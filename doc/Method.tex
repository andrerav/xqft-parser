%Relasjonsalgebra
\section{Method}
\subsection{Parser construction}
Writing a parser from scratch was ruled out early for being too time consuming.
Instead it was decided to use tools for compiler and parser construction to
generate a parser from the XQuery 1.0 and XPath 2.0 grammar specifications
developed by the W3C.

\subsection{Evaluated alternatives}
\subsubsection{JFlex/CUP}
JFlex and CUP is a versatile combination consisting of JFlex which is a lexer
generator, and CUP which is a parser generator. These tools can be interfaced to
generate a complete parser with a separate lexical analyzer.

JFlex and CUP produces only LALR parsers, and since the W3C has specified an
LL(1) grammar for XQuery 1.0 and XPath 2.0, the combination of JFlex and CUP was
rejected from this project.

\subsubsection{JavaCC}
JavaCC could have been a viable alternative as it produces LL(k) parsers,
however compared to Antlr its grammar specification syntax deviated so much
from the W3C EBNF syntax, that grammar would have had to be extensively
rewritten.

\subsubsection{Antlr}
Antlr is a renowned tool for parser generation, and can generate LL(k) parsers.
Additionally, Antlr accepts a grammar specification syntactically very close to
the EBNF used by the W3C. This is the parser generater chosen for our project,
based on the criteria outlined in this section.

\subsection{Limitations in Antlr}
\subsubsection{Unicode}
It is important to note, however, that Antlr has limited support for unicode.
In this project this implies that our parser will not accept unicode characters
in the range from and above 0x10000. This will exclude the supplementary
multilingual plane (SMP) range of unicode characters, as well as the
supplementary ideographic plane (SIP) and the supplementary special-purpose
plane (SSP). These are seldomly used, but this is an important limitation
nonetheless. The Antlr developers have indicated that support for this unicode
range will be added in future versions of Antlr.

As a remedy for this situation it is possible to couple an external lexer with
Antlr which will accept unicode characters in the above mentioned character
ranges. For the sake of simplicity this has not been further investigated nor
implemented in this project.

\subsection{Rewriting the grammar for Antlr syntax conformity}
\subsubsection{Lexer vs. parser syntax}
The Antlr parser generator can generate parsers and lexers from a single grammar
file. The distinction between terminals and non-terminals is simply a matter of
convention, where terminals are assumed to start with uppercase letters, and
non-terminals are assumed to start with lowercase letters.

In the grammar specified by the W3C, all the productions (terminals and
non-terminals) all start with uppercase letters. Initially this caused some
confusion, because this grammar naturally generated a very big lexer and a very
small and non-functional parser.

This was mitigated by converting non-terminal productions to start with
lowercase letters.

\subsubsection{Rewriting the W3C 'dash' operator}
In their specification, the W3c uses a dash operator, which has the following
semantic meaning in a grammar (from \cite{w3c03}, section 6):
\begin{quote}
A - B: matches any string that matches A but does not match B.
\end{quote}
This operator is not supported in Antlr, so it was necessary to rewrite
these productions using \emph{semantic predicates} where necessary. Thankfully,
in the original specification, the usage of the dash operator was rather sparse
and only used in trivial productions.

An example of rewriting the dash operator using semantic predicates:
\begin{verbatim}
// Original production
piTarget : Name - (('X' | 'x') ('M' | 'm') ('L' | 'l'));

// Rewritten production using a semantic predicate
piTarget : n=Name { !$n.getText().equalsIgnoreCase("XML") }?;
\end{verbatim}
The original production can be interpreted as ``piTarget can be a Name, but not
`XML', regardless of character casing''. The syntactic predicate will imitate
this behaviour using the method equalsIgnoreCase(). As can be seen from this
example, a semantic predicate is simply any kind of boolean Java expression.
This is a flexible solution, because the boolean expression can be wrapped in a
method with boolean return type, which for example then can be placed inside a
@members { } clause in the grammar file, or even as a static method in an
external class. This makes it possible to add complex grammar logic without
disturbing grammar brevity, if necessary.

\subsubsection{Grammar LL(1) conformity}
The grammar specification given by W3C is in a very compact and non-verbose
form, annoted with links to certain constraints and issues that need to be kept
in mind by anyone seeking to write a parser for XQuery and XPath. Here we will
list these constraints and briefly explain their implications for the parser.

\begin{itemize}
  \item 
\end{itemize}

\subsubsection{Reserved keywords}
A particular feature in XQuery is the lack of reserved keywords. This creates a
series of problems when a lexer based on the verbatim grammar specification from
the W3C is trying to recognize tokens. The culprit is the ambiguously defined
terminal symbols. Some of the base character tokens intersect with each other
and makes it hard if not impossible for the lexer to distinguish two different
tokens composed of different token fragments with intersecting characters.

One possible solution to this problem was to eliminate ambiguities in the lexer.
This approach was tried by finding and removing duplicate characters from token
fragments, and then generalizing the tokens and adding semantic predicates to
check for illegal and/or missing characters.

TODO: example

TODO: result (also note in conclusion)

\subsection{Unit testing the grammar}
Unit testing can be a powerful tool for asserting functionality and can be a
helpful aid in debugging and prevention of regression errors.  For unit testing the
grammar specification, gUnit \cite{gunit00} was employed. This tool uses a
syntax similar to Antlr itself, however instead of defining productions, one
defines a set of inputs for some rule, as well as the expected result. Consider
this example:

\begin{verbatim}
gunit XQFT;
@header{package no.ntnu.xqft.parse;}

piTarget: // Test piTarget rule

	// Any case permutation of 'XML' must fail
	"Xml" FAIL
	"XMl" FAIL
	"XML" FAIL
	"XmL" FAIL
\end{verbatim}

This is a complete input file for gUnit, and will automatically discover the
classes XQFTLexer and XQFTParser in the package no.ntnu.xqft.parse. gUnit will
then proceed to invoke the lexer with ``Xml'', ``XMl'', ``XML'', and ``XmL'' as
input, and pass the lexer to an instance of XQFTParser and execute the production
piTarget. For all these inputs, it will assert that the parser emits an error
(i.e it must fail to pass the test).

In case of a test where the parser should not fail, the syntax is as follows:
\begin{verbatim}
forClause:
	"for $a in document(\"abc.xml\")/a/b/text()" OK
\end{verbatim}
Here gUnit will assert that the parser will not fail for the given inpu (i.e it
must not fail to pass the test).

gUnit is also capable of parsing abstract syntax trees built by the generated
parser, but this feature has not been used in this project.
